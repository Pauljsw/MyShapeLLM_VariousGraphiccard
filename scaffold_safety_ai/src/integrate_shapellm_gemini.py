# LayerNorm ì°¨ì› ì˜¤ë¥˜ ìˆ˜ì • ë° Recon ëª¨ë¸ ì§ì ‘ ìƒì†
# scaffold_safety_ai/src/integrate_shapellm_gemini.py

import torch
import torch.nn as nn
import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any

# PointLoRA í•µì‹¬ ëª¨ë“ˆ import
from .pointlora_core import LoRALayer, SafetyTokenSelector

# ShapeLLMì˜ ReConV2 ëª¨ë“ˆì„ ë¡œë“œí•˜ê¸° ìœ„í•œ í™˜ê²½ ì„¤ì •
try:
    project_root = Path(__file__).resolve().parents[2]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        
    from ReConV2.models.ReCon import ReCon2
    from ReConV2.models.transformer import ReConBlocks, GPTExtractor
    from ReConV2.utils.checkpoint import get_missing_parameters_message, get_unexpected_parameters_message
    print(f"âœ… Successfully imported ReCon2 from: {project_root}")
except ImportError as e:
    print(f"âŒ Failed to import ReCon2. Check PYTHONPATH. Error: {e}")
    # ReCon2ê°€ ì—†ì„ ê²½ìš°, ë”ë¯¸ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´í•˜ì—¬ ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ í•¨
    class ReCon2(nn.Module):
        def __init__(self, config):
            super().__init__()
            print("âš ï¸ ReCon2 mock class is being used.")
            
            # ì‹¤ì œ ReCon ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ëª¨ë°©í•˜ì—¬ ì˜¤ë¥˜ë¥¼ íšŒí”¼
            self.model = nn.Module()
            self.model.encoder = GPTExtractor(
                embed_dim=config.embed_dim,
                num_heads=config.num_heads,
                depth=config.depth,
                group_size=config.group_size,
                drop_path_rate=config.drop_path_rate,
                stop_grad=config.stop_grad,
                pretrained_model_name=config.pretrained_model_name,
            )
            
            self.model.embed_dim = config.embed_dim
            self.model.inference = self._mock_inference

        def _mock_inference(self, pts):
            print("âš ï¸ ReCon2 mock inference is running. Returning random data.")
            local_features, global_features = self.model.encoder.forward(
                torch.randn(pts.size(0), 512, self.model.encoder.embed_dim),
                torch.randn(pts.size(0), 512, self.model.encoder.embed_dim),
                None,
                torch.randn(pts.size(0), 1, self.model.encoder.embed_dim),
            )
            return None, local_features, global_features


class PointLoRAReconEncoder(ReCon2):
    """
    ReCon2 ëª¨ë¸ì„ ì§ì ‘ ìƒì†ë°›ì•„ PointLoRAì™€ SafetyTokenSelectorë¥¼ í†µí•©í•˜ëŠ” í´ë˜ìŠ¤.
    ì´ í´ë˜ìŠ¤ëŠ” ì—°êµ¬ ë°©ë²•ë¡ ì˜ 'Aì•ˆ'ì„ ì™„ë²½í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    def __init__(self, config: dict):
        # ë¶€ëª¨ í´ë˜ìŠ¤(ReCon2)ì˜ ìƒì„±ìë¥¼ í˜¸ì¶œí•˜ì—¬ ê¸°ë³¸ Recon ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¡œë“œ
        super().__init__(config)
        self.config = config
        
        # LoRA ë ˆì´ì–´ë¥¼ Vision Transformer(ReCon++)ì˜ ë¸”ë¡ì— ì§ì ‘ ì£¼ì…
        self._add_lora_layers(
            lora_rank=config.lora_rank,
            lora_alpha=config.lora_alpha
        )
        
        # ì•ˆì „ í† í° ì„ íƒ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.safety_selector = SafetyTokenSelector(
            feature_dim=config.embed_dim,
            safety_token_count=config.safety_token_count
        )
        
        # ì•ˆì „ì„± ë“±ê¸‰ ë¶„ë¥˜ë¥¼ ìœ„í•œ í—¤ë“œ(Head)
        self.safety_classifier = nn.Sequential(
            nn.Linear(config.embed_dim, config.embed_dim // 2),
            nn.LayerNorm(config.embed_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(config.embed_dim // 2, 3) # 3: safe, warning, danger
        )
        
        print("âœ… PointLoRAReconEncoder initialized with LoRA and Safety Head.")

    def _add_lora_layers(self, lora_rank, lora_alpha):
        """
        ReCon2ì˜ MaskTransformer ì¸ì½”ë” ë¸”ë¡ì— LoRA ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ.
        """
        if not hasattr(self.model.encoder, 'blocks'):
            print("âš ï¸ Warning: Mock model encoder has no 'blocks' attribute. Skipping LoRA injection.")
            return

        # ReConBlocks ë‚´ë¶€ì˜ local_blocks(nn.Sequential)ì„ ìˆœíšŒí•´ì•¼ í•¨
        for i, block in enumerate(self.model.encoder.blocks.local_blocks):
            # Attention ë¸”ë¡ì˜ QKV í”„ë¡œì ì…˜ì— LoRA ì ìš©
            qkv_dim = block.attn.qkv.in_features
            block.attn.qkv_lora = LoRALayer(qkv_dim, qkv_dim * 3, lora_rank, lora_alpha)

            # FFN(Feed-Forward Network)ì˜ ì²« ë²ˆì§¸ Linear ë ˆì´ì–´ì— LoRA ì ìš©
            ffn_dim = block.mlp.fc1.in_features
            block.mlp.fc1_lora = LoRALayer(ffn_dim, block.mlp.fc1.out_features, lora_rank, lora_alpha)
            
            print(f"âœ… LoRA layers injected into Transformer block {i}")
            
    def load_pretrained_weights(self, ckpt_path: str, log: bool = True):
        """
        ì‚¬ì „ í›ˆë ¨ëœ ReCon ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ.
        """
        if not os.path.exists(ckpt_path):
            print(f"âŒ Checkpoint file not found: {ckpt_path}")
            return
            
        print(f"ğŸ“¦ Loading pre-trained weights from {ckpt_path}...")
        try:
            ckpt = torch.load(ckpt_path, map_location='cpu')
            
            # --- ìˆ˜ì •ëœ ë¶€ë¶„: 'state_dict'ì™€ 'base_model' ë‘ ê°€ì§€ í‚¤ ëª¨ë‘ ì‹œë„ ---
            state_dict = ckpt.get('state_dict', None)
            if state_dict is None:
                state_dict = ckpt.get('base_model', None)
                if state_dict is None:
                    raise KeyError("Neither 'state_dict' nor 'base_model' key found in checkpoint file.")

            # í‚¤ ë³€í™˜ ë¡œì§: 'module.' ë˜ëŠ” 'model.' í”„ë¦¬í”½ìŠ¤ ì œê±°
            clean_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace("module.", "")
                k = k.replace("model.", "")
                clean_state_dict[k] = v
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
            incompatible = self.model.load_state_dict(clean_state_dict, strict=False)
            
            if log:
                if incompatible.missing_keys:
                    print(f"âš ï¸ Missing Keys: {get_missing_parameters_message(incompatible.missing_keys)}")
                if incompatible.unexpected_keys:
                    print(f"âš ï¸ Unexpected Keys: {get_unexpected_parameters_message(incompatible.unexpected_keys)}")
            
            print(f"âœ… Pre-trained weights successfully loaded.")
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")

    def forward_safety_analysis(self, pts: torch.Tensor):
        """
        ì•ˆì „ ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ìˆœì „íŒŒ(forward) íŒŒì´í”„ë¼ì¸.
        """
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ inference ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ Recon íŠ¹ì§• ì¶”ì¶œ
        pos, local_features, global_features = self.model.inference(pts)

        # local_featuresëŠ” ReCon++ì˜ íŒ¨ì¹˜ë³„ íŠ¹ì§•ì´ë¯€ë¡œ, ì´ë¥¼ SafetyTokenSelectorì— ì „ë‹¬
        safety_tokens, safety_indices = self.safety_selector(local_features)
        
        # ì„ íƒëœ ì•ˆì „ í† í°ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë¶„ë¥˜ í—¤ë“œì— ì…ë ¥
        avg_safety_features = safety_tokens.mean(dim=1)
        safety_logits = self.safety_classifier(avg_safety_features)
        safety_probs = torch.softmax(safety_logits, dim=-1)
        
        return {
            'safety_tokens': safety_tokens,
            'safety_indices': safety_indices,
            'safety_logits': safety_logits,
            'safety_probs': safety_probs,
            'features': local_features,
            'predicted_class': torch.argmax(safety_probs, dim=-1),
            'confidence': torch.max(safety_probs, dim=-1)[0]
        }

    def set_training_mode(self, scaffold_training: bool = True):
        """
        í›ˆë ¨ ëª¨ë“œë¥¼ ì„¤ì •í•˜ì—¬ Base Modelì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³ 
        PointLoRAì™€ Safety Headì˜ ê°€ì¤‘ì¹˜ë§Œ í›ˆë ¨ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        """
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ê³ ì •
        for param in self.parameters():
            param.requires_grad = False
        
        if scaffold_training:
            # LoRAì™€ Safety Headì˜ íŒŒë¼ë¯¸í„°ë§Œ í™œì„±í™”
            for name, param in self.named_parameters():
                if 'lora' in name or 'safety_selector' in name or 'safety_classifier' in name:
                    param.requires_grad = True
        
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"ğŸ¯ Training mode set:")
        print(f"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.4f}%)")
    
    def _print_parameter_stats(self):
        """
        ì „ì²´ íŒŒë¼ë¯¸í„° ë° í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í†µê³„ ì¶œë ¥
        """
        total_params = sum(p.numel() for p in self.parameters())
        
        lora_params = sum(p.numel() for name, p in self.named_parameters() if 'lora' in name)
        safety_selector_params = sum(p.numel() for name, p in self.named_parameters() if 'safety_selector' in name)
        safety_classifier_params = sum(p.numel() for name, p in self.named_parameters() if 'safety_classifier' in name)
        
        print(f"ğŸ“Š Parameter Statistics:")
        print(f"   Total model params: {total_params:,}")
        print(f"   LoRA params: {lora_params:,}")
        print(f"   Safety Selector params: {safety_selector_params:,}")
        print(f"   Safety Classifier params: {safety_classifier_params:,}")
        print(f"   Total trainable params: {lora_params + safety_selector_params + safety_classifier_params:,}")

