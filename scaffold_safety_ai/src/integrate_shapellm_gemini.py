# scaffold_safety_ai/src/integrate_shapellm_gemini.py
# ìˆ˜ì •ëœ ë²„ì „: SafetyTokenSelector ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°

import torch
import torch.nn as nn
import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any

# PointLoRA í•µì‹¬ ëª¨ë“ˆ import
from .pointlora_core import LoRALayer

# ShapeLLMì˜ ReConV2 ëª¨ë“ˆì„ ë¡œë“œí•˜ê¸° ìœ„í•œ í™˜ê²½ ì„¤ì •
try:
    project_root = Path(__file__).resolve().parents[2]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        
    from ReConV2.models.ReCon import ReCon2
    from ReConV2.models.transformer import ReConBlocks, GPTExtractor
    from ReConV2.utils.checkpoint import get_missing_parameters_message, get_unexpected_parameters_message
    print(f"âœ… Successfully imported ReCon2 from: {project_root}")
except ImportError as e:
    print(f"âŒ Failed to import ReCon2. Check PYTHONPATH. Error: {e}")
    # ReCon2ê°€ ì—†ì„ ê²½ìš°, ë”ë¯¸ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´í•˜ì—¬ ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ í•¨
    class ReCon2(nn.Module):
        def __init__(self, config):
            super().__init__()
            print("âš ï¸ ReCon2 mock class is being used.")
            
            # ì‹¤ì œ ReCon ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ëª¨ë°©í•˜ì—¬ ì˜¤ë¥˜ë¥¼ íšŒí”¼
            self.model = nn.Module()
            self.model.encoder = GPTExtractor(
                embed_dim=config.embed_dim,
                num_heads=config.num_heads,
                depth=config.depth,
                group_size=config.group_size,
                drop_path_rate=config.drop_path_rate,
                stop_grad=config.stop_grad,
                pretrained_model_name=config.pretrained_model_name,
            )
            
            self.model.embed_dim = config.embed_dim
            self.model.inference = self._mock_inference

        def _mock_inference(self, pts):
            print("âš ï¸ ReCon2 mock inference is running. Returning random data.")
            local_features, global_features = self.model.encoder.forward(
                torch.randn(pts.size(0), 512, self.model.encoder.embed_dim),
                torch.randn(pts.size(0), 512, self.model.encoder.embed_dim),
                None,
                torch.randn(pts.size(0), 1, self.model.encoder.embed_dim),
            )
            return None, local_features, global_features


class FixedSafetyTokenSelector(nn.Module):
    """
    ìˆ˜ì •ëœ Safety Token Selector - ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, feature_dim: int = 1024, safety_token_count: int = 40):
        super().__init__()
        self.feature_dim = feature_dim
        self.safety_token_count = safety_token_count
        
        # ì¤‘ìš”ë„ ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬ - ì°¨ì›ì„ ëª…í™•íˆ ì„¤ì •
        self.importance_network = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.LayerNorm(feature_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim // 2, 1),
            nn.Sigmoid()
        )
        
        print(f"âœ… Safety Token Selector initialized: {feature_dim}D â†’ {safety_token_count} tokens")
        
    def forward(self, features: torch.Tensor):
        """
        ì•ˆì „ í† í° ì„ íƒ - ì°¨ì› ì•ˆì „ì„± í™•ë³´
        
        Args:
            features: [batch_size, seq_len, feature_dim] - ReConì˜ local_features
            
        Returns:
            safety_tokens: [batch_size, safety_token_count, feature_dim]
            selected_indices: [batch_size, safety_token_count]
        """
        batch_size, seq_len, feat_dim = features.shape
        
        # ë””ë²„ê¹… ì¶œë ¥
        print(f"ğŸ” [SafetyTokenSelector] Input shape: {features.shape}")
        print(f"ğŸ” [SafetyTokenSelector] Expected feature_dim: {self.feature_dim}, Got: {feat_dim}")
        
        # ì°¨ì› ê²€ì¦
        if feat_dim != self.feature_dim:
            print(f"âš ï¸ [SafetyTokenSelector] Feature dimension mismatch! Expected {self.feature_dim}, got {feat_dim}")
            # ì°¨ì› ì¡°ì • (ì„ì‹œ í•´ê²°ì±…)
            if feat_dim > self.feature_dim:
                features = features[:, :, :self.feature_dim]
                print(f"âœ… [SafetyTokenSelector] Truncated to {self.feature_dim} dimensions")
            else:
                # paddingìœ¼ë¡œ ì°¨ì› ë§ì¶”ê¸°
                padding = torch.zeros(batch_size, seq_len, self.feature_dim - feat_dim, 
                                    device=features.device, dtype=features.dtype)
                features = torch.cat([features, padding], dim=-1)
                print(f"âœ… [SafetyTokenSelector] Padded to {self.feature_dim} dimensions")
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        try:
            scores = self.importance_network(features).squeeze(-1)  # [B, S]
            print(f"ğŸ” [SafetyTokenSelector] Scores shape: {scores.shape}")
        except Exception as e:
            print(f"âŒ [SafetyTokenSelector] Error in importance_network: {e}")
            print(f"   Features shape: {features.shape}")
            raise e
        
        # Top-K ì„ íƒ
        k = min(self.safety_token_count, seq_len)
        _, indices = torch.topk(scores, k, dim=1)  # [B, K]
        
        # í† í° ì¶”ì¶œ
        batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, k)
        safety_tokens = features[batch_indices, indices]  # [B, K, D]
        
        print(f"âœ… [SafetyTokenSelector] Output safety_tokens shape: {safety_tokens.shape}")
        
        return safety_tokens, indices


class PointLoRAReconEncoder(ReCon2):
    """
    ReCon2 ëª¨ë¸ì„ ì§ì ‘ ìƒì†ë°›ì•„ PointLoRAì™€ SafetyTokenSelectorë¥¼ í†µí•©í•˜ëŠ” í´ë˜ìŠ¤.
    ì´ í´ë˜ìŠ¤ëŠ” ì—°êµ¬ ë°©ë²•ë¡ ì˜ 'Aì•ˆ'ì„ ì™„ë²½í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # PointLoRA íŒŒë¼ë¯¸í„° ì„¤ì •
        self.lora_rank = getattr(config, 'lora_rank', 16)
        self.lora_alpha = getattr(config, 'lora_alpha', 32)
        self.safety_token_count = getattr(config, 'safety_token_count', 40)
        
        # SafetyTokenSelector ì´ˆê¸°í™” - ReCon2ì˜ embed_dim ì‚¬ìš©
        self.safety_selector = FixedSafetyTokenSelector(
            feature_dim=config.embed_dim,
            safety_token_count=self.safety_token_count
        )
        
        # ì•ˆì „ ë¶„ë¥˜ í—¤ë“œ ì´ˆê¸°í™”
        self.safety_classifier = nn.Sequential(
            nn.Linear(config.embed_dim, config.embed_dim // 2),
            nn.LayerNorm(config.embed_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(config.embed_dim // 2, 5)  # 5ê°œì˜ ì•ˆì „ ë“±ê¸‰
        )
        
        # LoRA ë ˆì´ì–´ë¥¼ Transformer blocksì— ì£¼ì…
        self._inject_lora_layers()
        
        print(f"âœ… PointLoRAReconEncoder initialized with LoRA and Safety Head.")
    
    def _inject_lora_layers(self):
        """ReCon2ì˜ Transformer blocksì— LoRA ë ˆì´ì–´ ì£¼ì…"""
        try:
            # ReConBlocks êµ¬ì¡°: blocks.local_blocks (nn.Sequential)
            if not hasattr(self.model.encoder, 'blocks') or not hasattr(self.model.encoder.blocks, 'local_blocks'):
                print("âš ï¸ Warning: Expected ReConBlocks structure not found. Skipping LoRA injection.")
                return
                
            local_blocks = self.model.encoder.blocks.local_blocks
            print(f"ğŸ” Found {len(local_blocks)} local blocks in ReConBlocks")
            
            for i, block in enumerate(local_blocks):
                # MLPì˜ fc1ì— LoRA ì¶”ê°€
                if hasattr(block, 'mlp') and hasattr(block.mlp, 'fc1'):
                    in_features = block.mlp.fc1.in_features
                    out_features = block.mlp.fc1.out_features
                    block.mlp.fc1_lora = LoRALayer(in_features, out_features, self.lora_rank, self.lora_alpha)
                    print(f"âœ… LoRA Layer initialized: {in_features}â†’{out_features}, rank={self.lora_rank}, params={2*self.lora_rank*(in_features+out_features):,}")
                
                # MLPì˜ fc2ì— LoRA ì¶”ê°€ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
                if hasattr(block, 'mlp') and hasattr(block.mlp, 'fc2'):
                    in_features = block.mlp.fc2.in_features
                    out_features = block.mlp.fc2.out_features
                    block.mlp.fc2_lora = LoRALayer(in_features, out_features, self.lora_rank, self.lora_alpha)
                    print(f"âœ… LoRA Layer initialized: {in_features}â†’{out_features}, rank={self.lora_rank}, params={2*self.lora_rank*(in_features+out_features):,}")
                
                print(f"âœ… LoRA layers injected into Transformer block {i}")
                
        except Exception as e:
            print(f"âŒ Error injecting LoRA layers: {e}")
            import traceback
            traceback.print_exc()
            # LoRA ì£¼ì… ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ê¸°ë³¸ ëª¨ë¸ì€ ì‘ë™)
            print("âš ï¸ Continuing without LoRA layers...")
    
    def load_pretrained_weights(self, ckpt_path: str, log: bool = True):
        """
        ì‚¬ì „ í›ˆë ¨ëœ ReCon2 ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if not os.path.exists(ckpt_path):
            print(f"âŒ Checkpoint file not found: {ckpt_path}")
            return
            
        print(f"ğŸ“¦ Loading pre-trained weights from {ckpt_path}...")
        try:
            ckpt = torch.load(ckpt_path, map_location='cpu')
            
            # 'state_dict'ì™€ 'base_model' ë‘ ê°€ì§€ í‚¤ ëª¨ë‘ ì‹œë„
            state_dict = ckpt.get('state_dict', None)
            if state_dict is None:
                state_dict = ckpt.get('base_model', None)
                if state_dict is None:
                    raise KeyError("Neither 'state_dict' nor 'base_model' key found in checkpoint file.")

            # í‚¤ ë³€í™˜ ë¡œì§: 'module.' ë˜ëŠ” 'model.' í”„ë¦¬í”½ìŠ¤ ì œê±°
            clean_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace("module.", "")
                k = k.replace("model.", "")
                clean_state_dict[k] = v
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
            incompatible = self.model.load_state_dict(clean_state_dict, strict=False)
            
            if log:
                if incompatible.missing_keys:
                    print(f"âš ï¸ Missing Keys: {get_missing_parameters_message(incompatible.missing_keys)}")
                if incompatible.unexpected_keys:
                    print(f"âš ï¸ Unexpected Keys: {get_unexpected_parameters_message(incompatible.unexpected_keys)}")
            
            print(f"âœ… Pre-trained weights successfully loaded.")
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")

    def forward_safety_analysis(self, pts: torch.Tensor):
        """
        ì•ˆì „ ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ìˆœì „íŒŒ(forward) íŒŒì´í”„ë¼ì¸.
        ì°¨ì› ë””ë²„ê¹… í¬í•¨
        """
        print(f"ğŸ” [forward_safety_analysis] Input pts shape: {pts.shape}")
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ inference ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ Recon íŠ¹ì§• ì¶”ì¶œ
        pos, local_features, global_features = self.model.inference(pts)
        
        print(f"ğŸ” [forward_safety_analysis] local_features shape: {local_features.shape}")
        print(f"ğŸ” [forward_safety_analysis] global_features shape: {global_features.shape}")

        # local_featuresëŠ” ReCon++ì˜ íŒ¨ì¹˜ë³„ íŠ¹ì§•ì´ë¯€ë¡œ, ì´ë¥¼ SafetyTokenSelectorì— ì „ë‹¬
        safety_tokens, safety_indices = self.safety_selector(local_features)
        
        print(f"ğŸ” [forward_safety_analysis] safety_tokens shape: {safety_tokens.shape}")
        
        # ì„ íƒëœ ì•ˆì „ í† í°ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë¶„ë¥˜ í—¤ë“œì— ì…ë ¥
        avg_safety_features = safety_tokens.mean(dim=1)
        print(f"ğŸ” [forward_safety_analysis] avg_safety_features shape: {avg_safety_features.shape}")
        
        safety_logits = self.safety_classifier(avg_safety_features)
        safety_probs = torch.softmax(safety_logits, dim=-1)
        
        print(f"ğŸ” [forward_safety_analysis] safety_logits shape: {safety_logits.shape}")
        print(f"ğŸ” [forward_safety_analysis] safety_probs shape: {safety_probs.shape}")
        
        return {
            'safety_tokens': safety_tokens,
            'safety_indices': safety_indices,
            'safety_logits': safety_logits,
            'safety_probs': safety_probs,
            'features': local_features,
            'predicted_class': torch.argmax(safety_probs, dim=-1),
            'confidence': torch.max(safety_probs, dim=-1)[0]
        }

    def set_training_mode(self, scaffold_training: bool = True):
        """
        í›ˆë ¨ ëª¨ë“œë¥¼ ì„¤ì •í•˜ì—¬ Base Modelì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³ 
        PointLoRAì™€ Safety Headì˜ ê°€ì¤‘ì¹˜ë§Œ í›ˆë ¨ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        """
        if scaffold_training:
            # ì „ì²´ ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì • (ê¸°ë³¸ ê°€ì¤‘ì¹˜ ê³ ì •)
            self.eval()
            
            # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë¨¼ì € ë¹„í›ˆë ¨ìœ¼ë¡œ ì„¤ì •
            for param in self.parameters():
                param.requires_grad = False
            
            # LoRA íŒŒë¼ë¯¸í„°ë§Œ í›ˆë ¨ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
            trainable_params = 0
            total_params = 0
            
            for name, module in self.named_modules():
                if isinstance(module, LoRALayer):
                    module.train()
                    for param in module.parameters():
                        param.requires_grad = True
                        trainable_params += param.numel()
                
                # ëª¨ë“  íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                for param in module.parameters():
                    total_params += param.numel()
            
            # Safety ë¶„ë¥˜ í—¤ë“œë„ í›ˆë ¨ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
            self.safety_classifier.train()
            for param in self.safety_classifier.parameters():
                param.requires_grad = True
                trainable_params += param.numel()
            
            # Safety Token Selectorë„ í›ˆë ¨ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
            self.safety_selector.train()
            for param in self.safety_selector.parameters():
                param.requires_grad = True
                trainable_params += param.numel()
            
            print(f"ğŸ¯ Training mode set:")
            print(f"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.4f}%)")
            
        else:
            # ì „ì²´ ëª¨ë¸ í›ˆë ¨ ëª¨ë“œ
            self.train()
            for param in self.parameters():
                param.requires_grad = True