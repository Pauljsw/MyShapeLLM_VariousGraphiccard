# LayerNorm ì°¨ì› ì˜¤ë¥˜ ìˆ˜ì •
# scaffold_safety_ai/src/integrate_shapellm_fixed.py ì˜ SafetyTokenSelector ë¶€ë¶„ ìˆ˜ì •

import torch
import torch.nn as nn
import sys
import os
from pathlib import Path

# PointLoRA í•µì‹¬ ëª¨ë“ˆ import (ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)
from .pointlora_core import LoRALayer, SafetyTokenSelector

class SafeShapeLLMIntegration:
    """
    ì•ˆì „í•œ ShapeLLM í†µí•© ë°©ì‹
    ê¸°ì¡´ ì½”ë“œì˜ import ì˜¤ë¥˜ì™€ êµ¬ì¡°ì  ë¬¸ì œ í•´ê²°
    """
    
    def __init__(self, shapellm_path: str = None):
        self.shapellm_path = Path(shapellm_path) if shapellm_path else Path.cwd()
        self.setup_environment()
        
    def setup_environment(self):
        """ShapeLLM í™˜ê²½ ì•ˆì „í•˜ê²Œ ì„¤ì •"""
        try:
            # ShapeLLM ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
            if str(self.shapellm_path) not in sys.path:
                sys.path.insert(0, str(self.shapellm_path))
            
            # ì‹¤ì œ ShapeLLM ë””ë ‰í† ë¦¬ ì°¾ê¸°
            shapellm_dirs = [
                self.shapellm_path.parent,  # ìƒìœ„ ë””ë ‰í† ë¦¬ í™•ì¸
                self.shapellm_path.parent.parent,  # ë” ìƒìœ„ í™•ì¸
                Path("/home/aimgroup/ChoSW/mcp-server-demo/ShapeLLM")  # ì‹¤ì œ ê²½ë¡œ
            ]
            
            for potential_dir in shapellm_dirs:
                if (potential_dir / "llava").exists():
                    self.shapellm_path = potential_dir
                    if str(potential_dir) not in sys.path:
                        sys.path.insert(0, str(potential_dir))
                    print(f"âœ… Found ShapeLLM at: {potential_dir}")
                    break
            else:
                print(f"âš ï¸ ShapeLLM llava directory not found, using mock mode")
            
            return True
            
        except Exception as e:
            print(f"âŒ Environment setup failed: {e}")
            return False

class FixedSafetyTokenSelector(nn.Module):
    """
    ìˆ˜ì •ëœ Safety Token Selector - LayerNorm ì˜¤ë¥˜ í•´ê²°
    """
    
    def __init__(self, feature_dim: int = 768, safety_token_count: int = 40):
        super().__init__()
        self.feature_dim = feature_dim
        self.safety_token_count = safety_token_count
        
        # ìˆ˜ì •ëœ importance predictor - LayerNorm ìœ„ì¹˜ ë³€ê²½
        self.importance_network = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.LayerNorm(feature_dim // 2),  # ìž…ë ¥ ì°¨ì›ì— ë§žê²Œ ìˆ˜ì •
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim // 2, 1),
            nn.Sigmoid()
        )
        
        print(f"âœ… FixedSafetyTokenSelector initialized: {feature_dim}D â†’ {safety_token_count} tokens")
        
    def forward(self, features: torch.Tensor):
        """
        Safety token selection with fixed dimensions
        
        Args:
            features: [batch_size, seq_len, feature_dim]
            
        Returns:
            safety_tokens: [batch_size, safety_token_count, feature_dim]
            selected_indices: [batch_size, safety_token_count]
        """
        batch_size, seq_len, feat_dim = features.shape
        
        # Importance scoring with correct dimensions
        scores = self.importance_network(features).squeeze(-1)  # [B, S]
        
        # Top-K selection
        k = min(self.safety_token_count, seq_len)
        _, indices = torch.topk(scores, k, dim=1)
        
        # Extract tokens
        safety_tokens = torch.gather(
            features, 1, 
            indices.unsqueeze(-1).expand(-1, -1, feat_dim)
        )
        
        return safety_tokens, indices

class ScaffoldSafetyWrapper(nn.Module):
    """
    ìˆ˜ì •ëœ Scaffold Safety ëž˜í¼ - LayerNorm ì˜¤ë¥˜ í•´ê²°
    """
    
    def __init__(self, config: dict = None):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {
            'lora_rank': 16,
            'lora_alpha': 32,
            'safety_token_count': 40,
            'feature_dim': 768
        }
        
        # ìˆ˜ì •ëœ PointLoRA êµ¬ì„± ìš”ì†Œë“¤
        self.safety_selector = FixedSafetyTokenSelector(  # ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©
            feature_dim=self.config['feature_dim'],
            safety_token_count=self.config['safety_token_count']
        )
        
        # ìˆ˜ì •ëœ Safety classifier - LayerNorm ìœ„ì¹˜ ì¡°ì •
        self.safety_classifier = nn.Sequential(
            nn.Linear(self.config['feature_dim'], self.config['feature_dim'] // 2),
            nn.LayerNorm(self.config['feature_dim'] // 2),  # ì°¨ì› ë§žì¶¤
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.config['feature_dim'] // 2, 3)  # safe, warning, danger
        )
        
        # LoRA layers storage
        self.lora_layers = nn.ModuleDict()
        
        # ìˆ˜ì •ëœ Mock feature extractor
        self.mock_feature_extractor = self._create_mock_feature_extractor()
        
        print(f"âœ… ScaffoldSafetyWrapper initialized")
        self._print_parameter_stats()
    
    def _create_mock_feature_extractor(self):
        """ìˆ˜ì •ëœ Mock feature extractor"""
        return nn.Sequential(
            nn.Linear(6, 256),  # xyz+rgb input
            nn.LayerNorm(256),  # ì˜¬ë°”ë¥¸ ì°¨ì›
            nn.ReLU(),
            nn.Linear(256, self.config['feature_dim']),
            nn.LayerNorm(self.config['feature_dim'])  # ì˜¬ë°”ë¥¸ ì°¨ì›
        )
    
    # src/integrate_shapellm.py ì˜ add_lora_layer í•¨ìˆ˜ ìˆ˜ì •
    def add_lora_layer(self, layer_name: str, in_features: int, out_features: int):
        lora_layer = LoRALayer(
            in_features=in_features,
            out_features=out_features,
            rank=self.config['lora_rank'],
            alpha=self.config['lora_alpha']
        )
        # í•µì‹¬: nn.ModuleDictì— ë“±ë¡í•´ì•¼ parameters()ì— í¬í•¨ë¨
        self.lora_layers[layer_name] = lora_layer
        
        # ì¶”ê°€: ëª…ì‹œì ìœ¼ë¡œ parameter ë“±ë¡ í™•ì¸
        print(f"âœ… LoRA layer added: {layer_name}")
        print(f"   Parameters registered: {any(p.requires_grad for p in lora_layer.parameters())}")
    
    def forward_safety_analysis(self, point_cloud: torch.Tensor):
        """ìˆ˜ì •ëœ Safety analysis forward pass"""
        batch_size, num_points, point_dim = point_cloud.shape
        
        # 1. ì•ˆì „í•œ í¬ì¸íŠ¸ ì²˜ë¦¬
        if num_points > 512:
            # Simple downsampling for mock
            indices = torch.randperm(num_points)[:512]
            sampled_points = point_cloud[:, indices, :]
        else:
            sampled_points = point_cloud
            # Padding if necessary
            if sampled_points.shape[1] < 512:
                padding_size = 512 - sampled_points.shape[1] 
                padding = torch.zeros(batch_size, padding_size, point_dim)
                sampled_points = torch.cat([sampled_points, padding], dim=1)
        
        # 2. Mock feature extraction with correct dimensions
        features = self.mock_feature_extractor(sampled_points)  # [batch, 512, 768]
        
        # 3. Safety token selection
        safety_tokens, safety_indices = self.safety_selector(features)
        
        # 4. Safety classification - ì°¨ì› í™•ì¸
        avg_safety_features = safety_tokens.mean(dim=1)  # [batch, 768]
        
        # ì°¨ì› ë””ë²„ê¹…
        print(f"ðŸ” Debug: avg_safety_features shape: {avg_safety_features.shape}")
        
        safety_logits = self.safety_classifier(avg_safety_features)
        safety_probs = torch.softmax(safety_logits, dim=-1)
        
        return {
            'safety_tokens': safety_tokens,      # [batch, 40, 768] - Aì•ˆâ†’Bì•ˆ ì—°ê²°ìš©!
            'safety_indices': safety_indices,   # [batch, 40]
            'safety_logits': safety_logits,     # [batch, 3]
            'safety_probs': safety_probs,       # [batch, 3]
            'features': features,               # [batch, 512, 768]
            'predicted_class': torch.argmax(safety_probs, dim=-1),
            'confidence': torch.max(safety_probs, dim=-1)[0]
        }
    
    # src/integrate_shapellm.py ì˜ set_training_mode í•¨ìˆ˜ ìˆ˜ì •
    def set_training_mode(self, scaffold_training: bool = True):
        # ì „ì²´ ëª¨ë¸ freeze
        for param in self.parameters():
            param.requires_grad = False
        
        if scaffold_training:
            # ëª…ì‹œì ìœ¼ë¡œ LoRA íŒŒë¼ë¯¸í„°ë§Œ í™œì„±í™”
            for lora_layer in self.lora_layers.values():
                for param in lora_layer.parameters():
                    param.requires_grad = True
        
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"ðŸŽ¯ Training mode set:")
        print(f"   Trainable: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    
        # src/integrate_shapellm.py ì˜ _print_parameter_stats í•¨ìˆ˜ ìˆ˜ì •
    def _print_parameter_stats(self):
        total_params = sum(p.numel() for p in self.parameters())
        
        # ìˆ˜ì •: LoRA íŒŒë¼ë¯¸í„° ì •í™•ížˆ ê³„ì‚°
        lora_params = 0
        for lora_layer in self.lora_layers.values():
            lora_params += sum(p.numel() for p in lora_layer.parameters())
        
        safety_params = sum(p.numel() for name, p in self.named_parameters() 
                        if 'safety' in name)
        
        print(f"ðŸ“Š Parameter Statistics:")
        print(f"   Total: {total_params:,}")
        print(f"   LoRA: {lora_params:,}")
        print(f"   Safety: {safety_params:,}")
        print(f"   Trainable: {lora_params + safety_params:,}")

def create_scaffold_model(config: dict = None):
    """Scaffold Safety ëª¨ë¸ ìƒì„± í•¨ìˆ˜"""
    return ScaffoldSafetyWrapper(config)

# ============================================================================
# ìˆ˜ì •ëœ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ============================================================================

def test_fixed_integration():
    """ìˆ˜ì •ëœ í†µí•© ì½”ë“œ í…ŒìŠ¤íŠ¸"""
    print("ðŸ§ª Testing Fixed Integration...")
    
    # 1. í™˜ê²½ ì„¤ì • í…ŒìŠ¤íŠ¸
    integration = SafeShapeLLMIntegration()
    
    # 2. ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
    config = {
        'lora_rank': 16,
        'lora_alpha': 32,
        'safety_token_count': 40,
        'feature_dim': 768
    }
    
    model = create_scaffold_model(config)
    
    # 3. LoRA ë ˆì´ì–´ ì¶”ê°€ í…ŒìŠ¤íŠ¸
    model.add_lora_layer('attention_qkv', 768, 768*3)
    model.add_lora_layer('mlp_fc1', 768, 3072)
    
    # 4. í›ˆë ¨ ëª¨ë“œ ì„¤ì •
    model.set_training_mode(scaffold_training=True)
    
    # 5. Forward pass í…ŒìŠ¤íŠ¸ - ì°¨ì› ì•ˆì „ì„± í™•ë³´
    print("ðŸš€ Testing forward pass with dimension safety...")
    test_point_cloud = torch.randn(2, 8192, 6)  # batch=2, points=8192, xyz+rgb
    
    try:
        with torch.no_grad():
            results = model.forward_safety_analysis(test_point_cloud)
        
        print(f"âœ… Test Results:")
        print(f"   Safety tokens: {results['safety_tokens'].shape}")
        print(f"   Predicted classes: {results['predicted_class']}")
        print(f"   Confidence: {results['confidence']}")
        
        return model, results
        
    except Exception as e:
        print(f"âŒ Forward pass failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    # ìˆ˜ì •ëœ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    model, results = test_fixed_integration()
    if model is not None:
        print("âœ… Fixed integration test completed!")
    else:
        print("âŒ Integration test failed!")