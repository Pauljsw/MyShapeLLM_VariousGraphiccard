# scripts/train_stage_a.py
# Stage A í›ˆë ¨: PointLoRA-Only Training (ì—„ë°€í•œ êµ¬í˜„)

import torch
import torch.nn as nn
import torch.optim as optim
import json
import numpy as np
import time
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.integrate_shapellm import create_scaffold_model

class MockScaffoldDataset(Dataset):
    """Mock ë¹„ê³„ ì•ˆì „ ë°ì´í„°ì…‹ (ì—„ë°€í•œ êµ¬í˜„)"""
    
    def __init__(self, num_samples=1000, mode='train'):
        self.num_samples = num_samples
        self.mode = mode
        self.safety_levels = ['safe', 'warning', 'danger']
        self.safety_map = {'safe': 0, 'warning': 1, 'danger': 2}
        
        # ì¬í˜„ ê°€ëŠ¥í•œ ì‹œë“œ ì„¤ì •
        np.random.seed(42 if mode == 'train' else 123)
        torch.manual_seed(42 if mode == 'train' else 123)
        
        print(f"âœ… MockScaffoldDataset created: {num_samples} {mode} samples")
        
    def __len__(self):
        return self.num_samples
        
    def __getitem__(self, idx):
        # ì¬í˜„ ê°€ëŠ¥í•œ ë°ì´í„° ìƒì„±
        np.random.seed(idx + (42 if self.mode == 'train' else 123))
        torch.manual_seed(idx + (42 if self.mode == 'train' else 123))
        
        # Mock scaffold point cloud ìƒì„± (êµ¬ì¡°ì  íŠ¹ì„± ë°˜ì˜)
        point_cloud = self._generate_scaffold_structure()
        
        # Safety level ê²°ì • (í¸í–¥ëœ ë¶„í¬ë¡œ í˜„ì‹¤ì„± ì¦ê°€)
        if self.mode == 'train':
            safety_probs = [0.5, 0.3, 0.2]  # safeê°€ ë§ìŒ
        else:
            safety_probs = [0.4, 0.4, 0.2]  # ê²€ì¦ì—ì„œëŠ” ë” ê· ë“±
            
        safety_level = np.random.choice(self.safety_levels, p=safety_probs)
        safety_label = self.safety_map[safety_level]
        
        # ë†’ì´ ì •ë³´ (ì•ˆì „ë„ì— ë”°ë¥¸ í˜„ì‹¤ì  ë¶„í¬)
        if safety_level == 'safe':
            height = np.random.normal(105, 10)  # í‰ê·  105cm
        elif safety_level == 'warning':
            height = np.random.normal(88, 5)    # í‰ê·  88cm (ê¸°ì¤€ì„  ê·¼ì²˜)
        else:  # danger
            height = np.random.normal(75, 8)    # í‰ê·  75cm (ê¸°ì¤€ ë¯¸ë‹¬)
            
        height = max(50, min(150, height))  # í˜„ì‹¤ì  ë²”ìœ„ë¡œ ì œí•œ
        
        return {
            'point_cloud': point_cloud,
            'safety_label': torch.tensor(safety_label, dtype=torch.long),
            'height': torch.tensor(height, dtype=torch.float32),
            'sample_id': idx
        }
    
    def _generate_scaffold_structure(self):
        """êµ¬ì¡°ì  íŠ¹ì„±ì„ ë°˜ì˜í•œ ë¹„ê³„ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ìƒì„±"""
        points = []
        
        # 1. ìˆ˜ì§ ê¸°ë‘¥ë“¤ (4ê°œ ëª¨ì„œë¦¬)
        for x in [-1, 1]:
            for y in [-1, 1]:
                z_points = np.linspace(0, 3, 200)  # ë†’ì´ 3m
                noise = np.random.normal(0, 0.05, 200)
                column_points = np.column_stack([
                    np.full(200, x) + noise,
                    np.full(200, y) + noise,
                    z_points + noise
                ])
                points.append(column_points)
        
        # 2. ìˆ˜í‰ í”Œë«í¼ë“¤ (ì—¬ëŸ¬ ì¸µ)
        for z in [1.0, 2.0, 3.0]:  # 1m, 2m, 3m ë†’ì´
            x_grid, y_grid = np.meshgrid(
                np.linspace(-1, 1, 30),
                np.linspace(-1, 1, 30)
            )
            platform_points = np.column_stack([
                x_grid.flatten(),
                y_grid.flatten(),
                np.full(900, z) + np.random.normal(0, 0.02, 900)
            ])
            points.append(platform_points)
        
        # 3. ì—°ê²° ë¹”ë“¤ (ëŒ€ê°ì„ , ìˆ˜í‰)
        for _ in range(500):  # ì¶”ê°€ êµ¬ì¡° ìš”ì†Œë“¤
            start = np.random.uniform(-1, 1, 3)
            end = np.random.uniform(-1, 1, 3) 
            end[2] = start[2]  # ê°™ì€ ë†’ì´ë¡œ ì—°ê²°
            
            t = np.linspace(0, 1, 20)
            beam_points = np.outer(1-t, start) + np.outer(t, end)
            points.append(beam_points)
        
        # 4. ëª¨ë“  ì ë“¤ ê²°í•© ë° ì •ê·œí™”
        all_points = np.vstack(points)
        
        # 8192ê°œ ì ìœ¼ë¡œ ìƒ˜í”Œë§
        if len(all_points) > 8192:
            indices = np.random.choice(len(all_points), 8192, replace=False)
            sampled_points = all_points[indices]
        else:
            # ë¶€ì¡±í•˜ë©´ ë³µì œë¡œ ì±„ì›€
            needed = 8192 - len(all_points)
            extra_indices = np.random.choice(len(all_points), needed, replace=True)
            sampled_points = np.vstack([all_points, all_points[extra_indices]])
        
        # RGB ì •ë³´ ì¶”ê°€ (íšŒìƒ‰ ê¸ˆì† ìƒ‰ìƒ)
        colors = np.random.normal([0.5, 0.5, 0.5], 0.1, (8192, 3))
        colors = np.clip(colors, 0, 1)
        
        # XYZ + RGB = 6D
        point_cloud = np.hstack([sampled_points, colors]).astype(np.float32)
        
        return torch.tensor(point_cloud)

class StageATrainer:
    """Stage A í›ˆë ¨ í´ë˜ìŠ¤ (ì—„ë°€í•œ êµ¬í˜„)"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¯ Training device: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = create_scaffold_model(config).to(self.device)
        
        # LoRA ë ˆì´ì–´ ì¶”ê°€ (ì‹¤ì œ ShapeLLM êµ¬ì¡° ëª¨ë°©)
        self._add_lora_layers()
        
        # í›ˆë ¨ ëª¨ë“œ ì„¤ì • (LoRA + Safety ëª¨ë“ˆë§Œ)
        self.model.set_training_mode(scaffold_training=True)
        
        # Optimizer ë° Loss ì„¤ì •
        self._setup_training()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        
    def _add_lora_layers(self):
        """ì‹¤ì œ ShapeLLM Transformer êµ¬ì¡°ì— ë§ì¶˜ LoRA ì¶”ê°€"""
        print("ğŸ”§ Adding LoRA layers...")
        
        # ì¼ë°˜ì ì¸ Transformer êµ¬ì¡°ì˜ ì£¼ìš” ë ˆì´ì–´ë“¤
        lora_configs = [
            ('layer_0_attention_qkv', 768, 768*3),
            ('layer_0_mlp_fc1', 768, 3072),
            ('layer_1_attention_qkv', 768, 768*3),
            ('layer_1_mlp_fc1', 768, 3072),
            ('layer_2_attention_qkv', 768, 768*3),
            ('layer_2_mlp_fc1', 768, 3072),
            # ë” ë§ì€ ë ˆì´ì–´ ì¶”ê°€ ê°€ëŠ¥
        ]
        
        total_lora_params = 0
        for layer_name, in_dim, out_dim in lora_configs:
            self.model.add_lora_layer(layer_name, in_dim, out_dim)
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            rank = self.config['lora_rank']
            params = rank * (in_dim + out_dim)
            total_lora_params += params
        
        print(f"ğŸ“Š Total LoRA parameters: {total_lora_params:,}")
        
        # í›ˆë ¨ íš¨ìœ¨ì„± ê³„ì‚°
        total_params = sum(p.numel() for p in self.model.parameters())
        efficiency = (total_lora_params / total_params) * 100
        print(f"ğŸ“Š Training efficiency: {efficiency:.2f}% (Target: ~3.43%)")
        
    def _setup_training(self):
        """í›ˆë ¨ ì„¤ì •"""
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ìˆ˜ì§‘
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        # Optimizer
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        # Learning rate scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['num_epochs'],
            eta_min=self.config['learning_rate'] * 0.1
        )
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(
            weight=torch.tensor([1.0, 1.2, 1.5]).to(self.device)  # dangerì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        )
        
        print(f"âœ… Training setup complete:")
        print(f"   Trainable parameters: {sum(p.numel() for p in trainable_params):,}")  # ìˆ˜ì •
        print(f"   Learning rate: {self.config['learning_rate']}")
        print(f"   Weight decay: {self.config['weight_decay']}")
        
    def train_epoch(self, train_loader, epoch):
        """í•œ ì—í¬í¬ í›ˆë ¨ (ì—„ë°€í•œ êµ¬í˜„)"""
        self.model.train()
        
        epoch_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        batch_count = 0
        
        print(f"\nğŸ“š Epoch {epoch+1}/{self.config['num_epochs']}")
        print("-" * 50)
        
        for batch_idx, batch in enumerate(train_loader):
            point_clouds = batch['point_cloud'].to(self.device)
            safety_labels = batch['safety_label'].to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            
            results = self.model.forward_safety_analysis(point_clouds)
            safety_logits = results['safety_logits']
            
            # Loss ê³„ì‚°
            loss = self.criterion(safety_logits, safety_labels)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping (ì•ˆì •ì„± í–¥ìƒ)
            torch.nn.utils.clip_grad_norm_(
                [p for p in self.model.parameters() if p.requires_grad], 
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            epoch_loss += loss.item()
            predictions = torch.argmax(safety_logits, dim=1)
            correct_predictions += (predictions == safety_labels).sum().item()
            total_samples += safety_labels.size(0)
            batch_count += 1
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if batch_idx % 10 == 0:
                current_acc = correct_predictions / total_samples
                print(f"   Batch {batch_idx:3d}/{len(train_loader)}: "
                      f"Loss {loss.item():.4f}, Acc {current_acc:.3f}")
        
        # ì—í¬í¬ í‰ê· ê°’ ê³„ì‚°
        avg_loss = epoch_loss / batch_count
        accuracy = correct_predictions / total_samples
        
        self.train_losses.append(avg_loss)
        
        return avg_loss, accuracy
    
    def validate(self, val_loader, epoch):
        """ê²€ì¦ (ì—„ë°€í•œ êµ¬í˜„)"""
        self.model.eval()
        
        val_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        batch_count = 0
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì¶”ì 
        class_correct = torch.zeros(3)
        class_total = torch.zeros(3)
        
        with torch.no_grad():
            for batch in val_loader:
                point_clouds = batch['point_cloud'].to(self.device)
                safety_labels = batch['safety_label'].to(self.device)
                
                results = self.model.forward_safety_analysis(point_clouds)
                safety_logits = results['safety_logits']
                
                loss = self.criterion(safety_logits, safety_labels)
                
                val_loss += loss.item()
                predictions = torch.argmax(safety_logits, dim=1)
                correct_predictions += (predictions == safety_labels).sum().item()
                total_samples += safety_labels.size(0)
                batch_count += 1
                
                # í´ë˜ìŠ¤ë³„ ì •í™•ë„
                for i in range(3):
                    class_mask = (safety_labels == i)
                    if class_mask.sum() > 0:
                        class_correct[i] += (predictions[class_mask] == i).sum().item()
                        class_total[i] += class_mask.sum().item()
        
        avg_loss = val_loss / batch_count
        accuracy = correct_predictions / total_samples
        
        self.val_losses.append(avg_loss)
        self.val_accuracies.append(accuracy)
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì¶œë ¥
        print(f"   ğŸ“Š Class-wise accuracy:")
        class_names = ['Safe', 'Warning', 'Danger']
        for i, name in enumerate(class_names):
            if class_total[i] > 0:
                class_acc = class_correct[i] / class_total[i]
                print(f"      {name}: {class_acc:.3f} ({int(class_correct[i])}/{int(class_total[i])})")
        
        return avg_loss, accuracy
    
    def save_checkpoint(self, epoch, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'config': self.config
        }
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        checkpoint_dir = Path('checkpoints')
        checkpoint_dir.mkdir(exist_ok=True)
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        filename = 'stage_a_best.pth' if is_best else f'stage_a_epoch_{epoch}.pth'
        filepath = checkpoint_dir / filename
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filepath}")
        
        return filepath
    
    def extract_safety_tokens_sample(self, dataloader):
        """Aì•ˆâ†’Bì•ˆ ì—°ê²°ìš© Safety Tokens ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        self.model.eval()
        
        print("\nğŸ”— Testing Stage A â†’ B connection...")
        
        with torch.no_grad():
            batch = next(iter(dataloader))
            point_clouds = batch['point_cloud'][:2].to(self.device)  # 2 samples
            
            results = self.model.forward_safety_analysis(point_clouds)
            safety_tokens = results['safety_tokens']  # [2, 40, 768]
            safety_probs = results['safety_probs']
            
            print(f"   âœ… Safety tokens extracted: {safety_tokens.shape}")
            print(f"   âœ… Token statistics:")
            print(f"      Mean: {safety_tokens.mean().item():.4f}")
            print(f"      Std:  {safety_tokens.std().item():.4f}")
            print(f"      Min:  {safety_tokens.min().item():.4f}")
            print(f"      Max:  {safety_tokens.max().item():.4f}")
            print(f"   âœ… Predicted safety: {torch.argmax(safety_probs, dim=1)}")
            print(f"   âœ… Ready for Stage B integration!")
            
            return safety_tokens

def main():
    """Stage A í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stage A Training: PointLoRA-Only (ì—„ë°€í•œ êµ¬í˜„)")
    print("=" * 60)
    
    # í›ˆë ¨ ì„¤ì •
    config = {
        'lora_rank': 16,
        'lora_alpha': 32,
        'safety_token_count': 40,
        'feature_dim': 768,
        'learning_rate': 5e-4,
        'weight_decay': 1e-2,
        'batch_size': 4,
        'num_epochs': 20,
        'val_frequency': 2
    }
    
    print(f"ğŸ“‹ Training Configuration:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ“¦ Creating datasets...")
    train_dataset = MockScaffoldDataset(num_samples=800, mode='train')
    val_dataset = MockScaffoldDataset(num_samples=200, mode='val')
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"   Train: {len(train_dataset)} samples, {len(train_loader)} batches")
    print(f"   Val:   {len(val_dataset)} samples, {len(val_loader)} batches")
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    print(f"\nğŸ¯ Initializing trainer...")
    trainer = StageATrainer(config)
    
    # í›ˆë ¨ ë£¨í”„
    print(f"\nğŸƒ Starting training loop...")
    best_val_acc = 0.0
    start_time = time.time()
    
    for epoch in range(config['num_epochs']):
        # í›ˆë ¨
        train_loss, train_acc = trainer.train_epoch(train_loader, epoch)
        
        # ê²€ì¦ (ì¼ì • ê°„ê²©ìœ¼ë¡œ)
        if (epoch + 1) % config['val_frequency'] == 0:
            val_loss, val_acc = trainer.validate(val_loader, epoch)
            
            print(f"   ğŸ¯ Epoch {epoch+1} Summary:")
            print(f"      Train - Loss: {train_loss:.4f}, Acc: {train_acc:.3f}")
            print(f"      Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.3f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                trainer.save_checkpoint(epoch, is_best=True)
                print(f"      ğŸ‰ New best validation accuracy: {val_acc:.3f}")
            
            # Learning rate ì¡°ì •
            trainer.scheduler.step()
            current_lr = trainer.optimizer.param_groups[0]['lr']
            print(f"      ğŸ“ˆ Learning rate: {current_lr:.6f}")
    
    # í›ˆë ¨ ì™„ë£Œ
    training_time = time.time() - start_time
    print(f"\nâœ… Stage A Training Completed!")
    print(f"   Training time: {training_time/60:.1f} minutes")
    print(f"   Best validation accuracy: {best_val_acc:.3f}")
    
    # Aì•ˆâ†’Bì•ˆ ì—°ê²° í…ŒìŠ¤íŠ¸
    safety_tokens = trainer.extract_safety_tokens_sample(val_loader)
    
    # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    final_checkpoint = trainer.save_checkpoint(config['num_epochs']-1, is_best=False)
    
    print(f"\nğŸ¯ Stage A Results Summary:")
    print(f"   âœ… LoRA adaptation successful")
    print(f"   âœ… Safety token extraction working")
    print(f"   âœ… Ready for Stage B integration")
    print(f"   âœ… Best model saved: checkpoints/stage_a_best.pth")
    
    return trainer, safety_tokens

if __name__ == "__main__":
    trainer, safety_tokens = main()